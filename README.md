# Automatic Title Completion for Software Engineering Title Generation Tasks

## Introduction
![model](.\figs\model.jpg)

![model2](.\figs\model2.png)



Title quality is important for different software engineering communities. For example, in Stack Overflow, posts with low-quality question titles often discourage potential answerers. In GitHub, issues with low-quality titles can make it difficult for developers to grasp the core idea of the problem. In previous studies, researchers mainly focused on generating titles from scratch by analyzing the body contents, such as the post body for Stack Overflow question title generation (SOTG) and the issue body for issue title generation (ISTG). However, the quality of the generated titles is still limited by the information available in the body contents.  A more effective way is to provide accurate completion suggestions when developers compose titles. Inspired by this idea, we are the first to study the problem of automatic title completion for software engineering title generation tasks and proposed the approach TC4SETG. Specifically,  we first preprocess the gathered titles to form incomplete titles (i.e., tip information provided by developers) for simulating the title completion scene. Then we construct the input by concatenating the incomplete title with the body's content. Finally, we fine-tune the pre-trained model CodeT5 to effectively learn the title completion patterns. To evaluate the effectiveness of TC4SETG, we selected 164,748 high-quality posts from Stack Overflow by covering eight popular programming languages for the SOTG task  and 333,563 issues in the top-200 starred repositories on GitHub for the ISTG task. Our empirical results show that compared with the approaches of generating question titles from scratch, our proposed approach TC4SETG is more practical in automatic and human evaluation. Therefore, our study provides a new direction for studying automatic software engineering title generation and calls for more researchers to investigate this direction in the future.

## Pre-trained model and Datasets
If you want to download our pre-trained model and datasets, please [click this link](https://drive.google.com/drive/folders/1uH256MHVWFl4Q8jJgeJjaZarnb7I1kSP?usp=sharing)



## Results
The results generated by our model are saved to [this path](./results_and_metrics)<br>
We also shared our [script for computing metrics](./results_and_metrics/compute_metrics.py)<br>
Running the script directly and you should get the same results as in our paper



## How to train
Here is our [training script](./code/run.py).<br>
We have set the hyperparameters, you just need to change the path to save the model and the path to the dataset



## Tool

We developed a [browser plugin](./Plugin) tool based on TC4SETG to assist developers in composing titles. The video demonstration of using our tool is available at https://youtu.be/7WYB8K4rAtc)
